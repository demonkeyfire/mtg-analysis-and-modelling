{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to process and upload keyword data from MTGJSON into the postgresql database mtg_db. This is done through the following steps:\n",
    "- Download the json file from MTGJSON's file server\n",
    "- Check the version and date of the json file\n",
    "- Pre-process the dictionary and convert it into a dataframe\n",
    "- Push the keywords dataframe to the database \"raw_data\" schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column           | Renamed   | Dataype | Description                                              |\n",
    "| ---              | ---       | ---     | ---                                                      |\n",
    "| abilityWords     | ABILITIES | STRING  | A list of ability words found in rules text on cards     |\n",
    "| keywordAbilities | KEYWORDS  | STRING  | A list of keyword abilities found in rules text on cards |\n",
    "| keywordActions   | ACTIONS   | STRING  | A list of keyword actiona found in rules text on cards   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import lzma\n",
    "from   tqdm       import tqdm\n",
    "import numpy      as     np\n",
    "import pandas     as     pd\n",
    "from   sqlalchemy import create_engine, text\n",
    "\n",
    "## Modular functions\n",
    "# Setting the root path for finding the modules directory\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "# Loading Modular functions\n",
    "from   modules.data_recency import data_recency_check, recency_check_upload\n",
    "\n",
    "# Clean-Up\n",
    "del sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all columns instead of truncating with \"...\"\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# (Optional) also show all rows\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# (Optional) widen the display area so columns don’t wrap badly\n",
    "pd.set_option(\"display.width\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up credentials for accessing postgresql \"mtg_db\" database\n",
    "\n",
    "# Credentials for setting up connection to postgresql\n",
    "user     = \"postgres\"\n",
    "password = \"as:123bpostgresql\"\n",
    "host     = \"localhost\"\n",
    "port     = \"5432\"\n",
    "database = \"mtg_db\"\n",
    "\n",
    "# Engine connection to postgresql\n",
    "engine = create_engine(f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}\")\n",
    "\n",
    "# Clean-Up\n",
    "del user, password, host, port, database, create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the empty data_recency table if not exists\n",
    "query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS raw_data.data_recency (\n",
    "             json_type      TEXT PRIMARY KEY\n",
    "            ,latest_date    DATE\n",
    "            ,latest_version TEXT);\n",
    "        \"\"\"\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(query))\n",
    "\n",
    "    # Clean-Up\n",
    "    del query, conn, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 2.02k/2.02k [00:00<00:00, 25.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "# URL for MTGJSON (example: Keywords.xz)\n",
    "url = \"https://mtgjson.com/api/v5/Keywords.json.xz\"\n",
    "\n",
    "# Download the compressed file\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Prepare to track total size and read in chunks\n",
    "total_size = int(response.headers.get('content-length', 0))  # total bytes, may be None\n",
    "chunk_size = 1024 * 1024  # 1 MB per chunk\n",
    "compressed_data = bytearray()  # store the downloaded bytes\n",
    "\n",
    "# Iterate over response chunks, updating progress bar\n",
    "with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as pbar:\n",
    "    for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "        if chunk:  # filter out keep-alive chunks\n",
    "            compressed_data.extend(chunk)\n",
    "            pbar.update(len(chunk))\n",
    "\n",
    "# Decompress the .xz file\n",
    "decompressed_bytes = lzma.decompress(compressed_data)\n",
    "\n",
    "# Parse JSON into a dictionary\n",
    "dict__keywords = json.loads(decompressed_bytes)\n",
    "\n",
    "# Clean-Up\n",
    "del url, tqdm, total_size, chunk_size, lzma, chunk, json, requests\n",
    "del response, compressed_data, decompressed_bytes, pbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the latest version of the input data\n",
    "df__data_recency = data_recency_check(dict__keywords, 'keyword')\n",
    "\n",
    "# Clean-Up\n",
    "del data_recency_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting the dictionary to a dataframe, renaming the columns and making empty values empty strings\n",
    "\n",
    "# Converting the json dictionary to a dataframe\n",
    "df__keywords = pd.DataFrame.from_dict(dict__keywords['data']\n",
    "                                     # The columns are different lengths\n",
    "                                     ,orient = 'index').transpose()\n",
    "\n",
    "# Renaming the columns\n",
    "df__keywords.columns = ['abilities'\n",
    "                       ,'keywords'\n",
    "                       ,'actions']\n",
    "\n",
    "# Sort each column independently, pushing NaNs and empty strings to the bottom\n",
    "df__keywords = df__keywords.apply(lambda col: col.replace('', np.nan)             # Treat empty strings as NaN\n",
    "                                                 .sort_values(na_position='last') # Sort values\n",
    "                                                 .fillna('')                      # Put empty strings back if desired\n",
    "                                                 .values)                         # Reset index\n",
    "\n",
    "# Clean-Up\n",
    "del dict__keywords, np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending/replacing the meta data of the json download to a central table\n",
    "recency_check_upload(schema_name = \"raw_data\"\n",
    "                    ,table_name  = \"data_recency\"\n",
    "                    ,dataframe   = df__data_recency\n",
    "                    ,engine = engine)\n",
    "\n",
    "# Clean-Up\n",
    "del df__data_recency, recency_check_upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading the keywords dataframe to postgresql\n",
    "df__keywords.to_sql(name      = \"keywords\"\n",
    "                   ,con       = engine\n",
    "                   ,schema    = \"raw_data\"\n",
    "                   ,if_exists = \"replace\"\n",
    "                   ,index     = False)\n",
    "\n",
    "# Clean-Up\n",
    "del df__keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>json_type</th>\n",
       "      <th>latest_date</th>\n",
       "      <th>latest_version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all printings</td>\n",
       "      <td>2025-09-08</td>\n",
       "      <td>5.2.2+20250908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>set list</td>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>5.2.2+20250924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>keyword</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>5.2.2+20250928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       json_type latest_date  latest_version\n",
       "0  all printings  2025-09-08  5.2.2+20250908\n",
       "1       set list  2025-09-24  5.2.2+20250924\n",
       "2        keyword  2025-09-28  5.2.2+20250928"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the json file date and version\n",
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM raw_data.data_recency\n",
    "        \"\"\"\n",
    "pd.read_sql_query(query, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataframe top 10 values\n",
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM raw_data.keywords\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "pd.read_sql_query(query, con=engine)\n",
    "\n",
    "# Clean-Up\n",
    "del engine, pd, query"
   ]
  }
 ],
 "metadata": {
  "hex_info": {
   "author": "Adam Brown",
   "exported_date": "Fri Aug 15 2025 20:57:54 GMT+0000 (Coordinated Universal Time)",
   "project_id": "01982c18-3640-7001-8127-b56bed0a428f",
   "version": "draft"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

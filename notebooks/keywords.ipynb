{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to process and upload keyword data from MTGJSON into the postgresql database mtg_db. This is done through the following steps:\n",
    "- Download the json file from MTGJSON's file server\n",
    "- Check the version and date of the json file\n",
    "- Pre-process the dictionary and convert it into a dataframe\n",
    "- Push the keywords dataframe to the database \"raw_data\" schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column           | Renamed   | Dataype | Description                                              |\n",
    "| ---              | ---       | ---     | ---                                                      |\n",
    "| abilityWords     | abilities | STRING  | A list of ability words found in rules text on cards     |\n",
    "| keywordAbilities | keywords  | STRING  | A list of keyword abilities found in rules text on cards |\n",
    "| keywordActions   | actions   | STRING  | A list of keyword actiona found in rules text on cards   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import lzma\n",
    "from   tqdm       import tqdm\n",
    "import numpy      as     np\n",
    "import pandas     as     pd\n",
    "from   sqlalchemy import create_engine, Table, Column, MetaData, Text, Date, text\n",
    "from   sqlalchemy.dialects.postgresql import insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all columns instead of truncating with \"...\"\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# (Optional) also show all rows\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# (Optional) widen the display area so columns don’t wrap badly\n",
    "pd.set_option(\"display.width\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for showing the data and version of the MTGJSON data\n",
    "def data_recency_check(data, json_type):\n",
    "\n",
    "    \"\"\"\n",
    "    Extract and display the version and date metadata from an MTGJSON dataset,\n",
    "    and return this information as a DataFrame along with the JSON type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        MTGJSON data loaded from a JSON file, expected to contain a 'meta' key\n",
    "        with 'date' and 'version' fields.\n",
    "\n",
    "    json_type : str\n",
    "        A string indicating the type or name of the JSON dataset being processed.\n",
    "        This will be included in the output DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with a single row and columns:\n",
    "        - 'json_type': The provided JSON dataset type/name.\n",
    "        - 'latest_date': The date the MTGJSON data was last updated.\n",
    "        - 'latest_version': The MTGJSON model version.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a DataFrame for the output\n",
    "    df = pd.DataFrame({'json_type'      : [json_type]\n",
    "                      ,'latest_date'    : [data['meta']['date']]\n",
    "                      ,'latest_version' : [data['meta']['version']]})\n",
    "\n",
    "    # Returning the values directly\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for uploading the recency check\n",
    "\n",
    "def recency_check_upload(schema_name, table_name, dataframe):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uploads recency check data from a Pandas DataFrame into a PostgreSQL table \n",
    "    with upsert (insert or update) logic.\n",
    "\n",
    "    Each row from the DataFrame is inserted into the target table. If a row with the \n",
    "    same `json_type` (primary key) already exists, the corresponding `latest_date` \n",
    "    and `latest_version` values are updated instead.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    schema_name : str\n",
    "        Name of the PostgreSQL schema where the table resides.\n",
    "    table_name : str\n",
    "        Name of the PostgreSQL table to update or insert into.\n",
    "    dataframe : pandas.DataFrame\n",
    "        DataFrame containing the recency check data with columns:\n",
    "        - 'json_type' (str): Identifier for the JSON file type.\n",
    "        - 'latest_date' (datetime.date): Date of the latest file.\n",
    "        - 'latest_version' (str): Version string of the latest file.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Requires a global SQLAlchemy `engine` object to be defined.\n",
    "    - Uses PostgreSQL's ON CONFLICT clause for upsert behavior.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a MetaData object\n",
    "    metadata = MetaData(schema=schema_name)\n",
    "    \n",
    "    # Define the Table object matching your PostgreSQL table\n",
    "    json_recency_table = Table(table_name\n",
    "                              ,metadata\n",
    "                              ,Column('json_type' ,Text ,primary_key = True)\n",
    "                              ,Column('latest_date' ,Date)\n",
    "                              ,Column('latest_version' ,Text))\n",
    "    \n",
    "    # Upsert each row from your DataFrame\n",
    "    with engine.begin() as conn:\n",
    "        \n",
    "        # Iterate through rows of the DataFrame\n",
    "        for _, row in dataframe.iterrows():\n",
    "            \n",
    "            # Create an insert statement for the current row\n",
    "            stmt = insert(json_recency_table).values(json_type      = row['json_type']\n",
    "                                                    ,latest_date    = row['latest_date']\n",
    "                                                    ,latest_version = row['latest_version'])\n",
    "            \n",
    "            # Add upsert logic to update on conflict\n",
    "            stmt = stmt.on_conflict_do_update(index_elements = ['json_type']\n",
    "                                             ,set_           = {'latest_date'    : row['latest_date']\n",
    "                                                               ,'latest_version' : row['latest_version']})\n",
    "            \n",
    "            # Execute the statement\n",
    "            conn.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for showing the hierarchy of a dictionary or the schema of a single level\n",
    "def print_dict_structure(data, max_depth=None, _indent=0):\n",
    "\n",
    "    \"\"\"\n",
    "    Recursively prints the hierarchical structure of a dictionary or list,\n",
    "    including the length of each element where applicable.\n",
    "    \n",
    "    If max_depth=1, returns a DataFrame with columns: KEY_NAME, DATA_TYPE, LENGTH.\n",
    "    \n",
    "    Args:\n",
    "        data: The dictionary or list to explore.\n",
    "        max_depth: Limit how deep to traverse (None for full depth).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame if max_depth=1, otherwise None (prints output).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if we are at the top level and max_depth=1 to return DataFrame instead of printing\n",
    "    if max_depth == 1 and _indent == 0:\n",
    "        # Initialize list to collect rows for the DataFrame\n",
    "        rows = []\n",
    "\n",
    "        # If data is a dictionary, iterate over its keys and values\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                # Determine length if possible, otherwise set to 0\n",
    "                length = len(value) if hasattr(value, \"__len__\") and not isinstance(value, (str, bytes)) else 0\n",
    "                # Append a tuple of key name, data type, and length to rows\n",
    "                rows.append((key, type(value).__name__, length))\n",
    "\n",
    "        # If data is a list, take the first element (assumed dict) and do the same\n",
    "        elif isinstance(data, list) and data:\n",
    "            for key, value in data[0].items():\n",
    "                # Determine length if possible, otherwise set to 0\n",
    "                length = len(value) if hasattr(value, \"__len__\") and not isinstance(value, (str, bytes)) else 0\n",
    "                # Append a tuple of key name, data type, and length to rows\n",
    "                rows.append((key, type(value).__name__, length))\n",
    "\n",
    "        # Convert collected rows into a DataFrame with specific column names\n",
    "        return pd.DataFrame(rows, columns=[\"KEY_NAME\", \"DATA_TYPE\", \"LENGTH\"])\n",
    "    \n",
    "    # Create a prefix for indentation when printing nested structures\n",
    "    prefix = \"  \" * _indent\n",
    "\n",
    "    # If data is a dictionary, iterate recursively\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            # Prepare a string showing type and length for printing\n",
    "            length_info = f\", len={len(value)}\" if hasattr(value, \"__len__\") and not isinstance(value, (str, bytes)) else \"\"\n",
    "            # Print the key name, type, and length with proper indentation\n",
    "            print(f\"{prefix}{key} ({type(value).__name__}{length_info})\")\n",
    "            # Recurse into value if max_depth is not reached\n",
    "            if max_depth is None or _indent < max_depth - 1:\n",
    "                print_dict_structure(value, max_depth, _indent + 1)\n",
    "\n",
    "    # If data is a list, recurse into the first element (assuming homogeneous elements)\n",
    "    elif isinstance(data, list):\n",
    "        if data and (max_depth is None or _indent < max_depth - 1):\n",
    "            print_dict_structure(data[0], max_depth, _indent + 1)\n",
    "\n",
    "    # For non-dict and non-list elements, print their type and length\n",
    "    else:\n",
    "        # Prepare a string showing type and length for printing\n",
    "        length_info = f\", len={len(data)}\" if hasattr(data, \"__len__\") and not isinstance(data, (str, bytes)) else \"\"\n",
    "        # Print the type and length with proper indentation\n",
    "        print(f\"{prefix}{type(data).__name__}{length_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up credentials for accessing postgresql \"mtg_db\" database\n",
    "\n",
    "# Credentials for setting up connection to postgresql\n",
    "user     = \"postgres\"\n",
    "password = \"as:123bpostgresql\"\n",
    "host     = \"localhost\"\n",
    "port     = \"5432\"\n",
    "database = \"mtg_db\"\n",
    "\n",
    "# Engine connection to postgresql\n",
    "engine = create_engine(f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the empty data_recency table if not exists\n",
    "query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS raw_data.data_recency (\n",
    "             json_type      TEXT PRIMARY KEY\n",
    "            ,latest_date    DATE\n",
    "            ,latest_version TEXT);\n",
    "        \"\"\"\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 2.02k/2.02k [00:00<00:00, 28.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "# URL for MTGJSON (example: Keywords.xz)\n",
    "url = \"https://mtgjson.com/api/v5/Keywords.json.xz\"\n",
    "\n",
    "# Download the compressed file\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Prepare to track total size and read in chunks\n",
    "total_size = int(response.headers.get('content-length', 0))  # total bytes, may be None\n",
    "chunk_size = 1024 * 1024  # 1 MB per chunk\n",
    "compressed_data = bytearray()  # store the downloaded bytes\n",
    "\n",
    "# Iterate over response chunks, updating progress bar\n",
    "with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as pbar:\n",
    "    for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "        if chunk:  # filter out keep-alive chunks\n",
    "            compressed_data.extend(chunk)\n",
    "            pbar.update(len(chunk))\n",
    "\n",
    "# Decompress the .xz file\n",
    "decompressed_bytes = lzma.decompress(compressed_data)\n",
    "\n",
    "# Parse JSON into a dictionary\n",
    "dict__keywords = json.loads(decompressed_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the latest version of the input data\n",
    "df__data_recency = data_recency_check(dict__keywords, 'keyword')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting the dictionary to a dataframe, renaming the columns and making empty values empty strings\n",
    "\n",
    "# Converting the json dictionary to a dataframe\n",
    "df__keywords = pd.DataFrame.from_dict(dict__keywords['data']\n",
    "                                     # The columns are different lengths\n",
    "                                     ,orient = 'index').transpose()\n",
    "\n",
    "# Renaming the columns\n",
    "df__keywords.columns = ['abilities'\n",
    "                       ,'keywords'\n",
    "                       ,'actions']\n",
    "\n",
    "# Sort each column independently, pushing NaNs and empty strings to the bottom\n",
    "df__keywords = df__keywords.apply(lambda col: col.replace('', np.nan)             # Treat empty strings as NaN\n",
    "                                                 .sort_values(na_position='last') # Sort values\n",
    "                                                 .fillna('')                      # Put empty strings back if desired\n",
    "                                                 .values)                         # Reset index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_check_upload(schema_name = \"raw_data\"\n",
    "                    ,table_name  = \"data_recency\"\n",
    "                    ,dataframe   = df__data_recency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uploading the keywords dataframe to postgresql\n",
    "df__keywords.to_sql(name      = \"keywords\"\n",
    "                   ,con       = engine\n",
    "                   ,schema    = \"raw_data\"\n",
    "                   ,if_exists = \"replace\"\n",
    "                   ,index     = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>json_type</th>\n",
       "      <th>latest_date</th>\n",
       "      <th>latest_version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keyword</td>\n",
       "      <td>2025-09-15</td>\n",
       "      <td>5.2.2+20250915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  json_type latest_date  latest_version\n",
       "0   keyword  2025-09-15  5.2.2+20250915"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the json file date and version\n",
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM raw_data.data_recency\n",
    "        WHERE json_type = 'keyword'\n",
    "        \"\"\"\n",
    "pd.read_sql_query(query, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abilities</th>\n",
       "      <th>keywords</th>\n",
       "      <th>actions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adamant</td>\n",
       "      <td>Absorb</td>\n",
       "      <td>Abandon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Addendum</td>\n",
       "      <td>Affinity</td>\n",
       "      <td>Activate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alliance</td>\n",
       "      <td>Afflict</td>\n",
       "      <td>Adapt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Battalion</td>\n",
       "      <td>Afterlife</td>\n",
       "      <td>Amass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bloodrush</td>\n",
       "      <td>Aftermath</td>\n",
       "      <td>Assemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Celebration</td>\n",
       "      <td>Amplify</td>\n",
       "      <td>Attach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Channel</td>\n",
       "      <td>Annihilator</td>\n",
       "      <td>Behold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chroma</td>\n",
       "      <td>Ascend</td>\n",
       "      <td>Bolster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cohort</td>\n",
       "      <td>Assist</td>\n",
       "      <td>Cast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Constellation</td>\n",
       "      <td>Augment</td>\n",
       "      <td>Clash</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       abilities     keywords   actions\n",
       "0        Adamant       Absorb   Abandon\n",
       "1       Addendum     Affinity  Activate\n",
       "2       Alliance      Afflict     Adapt\n",
       "3      Battalion    Afterlife     Amass\n",
       "4      Bloodrush    Aftermath  Assemble\n",
       "5    Celebration      Amplify    Attach\n",
       "6        Channel  Annihilator    Behold\n",
       "7         Chroma       Ascend   Bolster\n",
       "8         Cohort       Assist      Cast\n",
       "9  Constellation      Augment     Clash"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the dataframe top 10 values\n",
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM raw_data.keywords\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "pd.read_sql_query(query, con=engine)"
   ]
  }
 ],
 "metadata": {
  "hex_info": {
   "author": "Adam Brown",
   "exported_date": "Fri Aug 15 2025 20:57:54 GMT+0000 (Coordinated Universal Time)",
   "project_id": "01982c18-3640-7001-8127-b56bed0a428f",
   "version": "draft"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
